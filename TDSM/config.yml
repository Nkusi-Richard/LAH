# Configuration parameters

# ---------------------------------------------------------------------------
# PROCESS PARAMETERS
process:
    # Specify GPU to use, if set to None, CPU will be used (int)
    cuda: 1

    # Activate Debug mode (generate a 'test' environment and folder
    # for visdom and report output)
    debug: False

    # Activate report mode, this would generate a complete report
    # from prediction done on the evalutation set
    report: False

    # Activate inference mode:
    inference: False

    # Output path to save checkpoints and reports
    output_path: './output'

visualization:
    # Set specific port for visdom writer (int)
    visdom_port: 8097

    # Set one specific label to generate the overlayed of the class prediction (str)
    lbl_visu:

    # Activate the plot display
    verbose: False

    # Specify custom label colors
    # e.g. {'class_1: [125, 125, 0]}
    palette:
            {Unclassified: [0, 0, 0],
             no_damage: [255, 255, 0],
             central_zone: [255,0,0],
             ring_zone: [0,149,255]}


# ---------------------------------------------------------------------------
# SPECTRUM EXPLORATION
spectrum_exploration:
    # Generate data visualization
    with_exploration: False

    # Scale used to plot spectrums curves (list[float, float])
    plot_scale: [0, 1]

    # Class of interest, display their std on the spectral curves (list[str])
    curve_class: []


# ----------------------------------------------------------------------------
#  DATASET PARAMETERS
dataset:
    # Dataset name (str)
    dataset_name: Laser

    loading:
        # Path to dataset folder (str)
        folder: /home/rnkusi/codes/V10_not

        # Path to class index file (str)
        class_index:  /home/rnkusi/codes/V10_not/laser.txt

        # Data template names
        img_tmpl: '.dat'
        mask_tmpl: '.png'

        # Specify data order (list[str])
        # (by default, an ascending sort is applied on names)
        file_order:

        # Additional optional keys to load custom datasets
        custom_data_variables: {'subsets': [ 'PATIENT_pig_1_ROI_1_30-08-2019',
        'PATIENT_pig_1_ROI_2_30-08-2019',
        'PATIENT_pig_1_ROI_5_30-08-2019',
        'PATIENT_pig_1_ROI_6_30-08-2019',
        'PATIENT_pig_1_ROI_7_30-08-2019',
                                             'PATIENT_pig_1_ROI_8_30-08-2019'
],
                                'shuffle_subsets': False,
                                'annotation': 'reg',
                                'raw_data': True}

        # Labels to be ignored during the process (list[str])
        ignored_labels:
                - Unclassified

        # Labels to be fused during the process, please specify the new name as a key (dict)
        # e.g. fused_labels: {'New_name': ['Label_1', 'Label_2'],
        #                      'New_name_2': ['Label_3', 'Label_4']}
        fused_labels:

        # The train/test split is generated by default according to the
        # sampling setup, it is possible to manually specify the split by specifying
        # the respective folders though.
        # Path to the train ground truth (str)
        train_set:
        # Path to the test set (str)
        test_set:

    # ----------------------------
    # SAMPLING OPTIONS
    # Options to setup the train/test split
    sampling:
        # Sampling mode, e.g. random sampling (str)
        # Choose among: {'disjoint', 'cross_val', 'random'}
        mode: cross_val

        # Samples proportion to use for training for 'random' & 'disjoint' modes ([0, 1] float)
        proportion: 0.7

        # Number of k folds for cross-validation (int)
        kfolds: 6 


    # ----------------------------
    # DATA NORMALIZATION
    normalization:
        # Normalize the hypercube
        normalized: False

        # Standardize the hypercube
        standardized: False

        # Threshold to be applied to the hypercube values before normalization (float)
        norm_thresh:

        # Apply channel wised normalization on hypercube
        channel_norm: False

        # Apply class normalization on hypercube, please specify the desired class (str)
        class_norm:

        # Apply standard normal variate normalization on the hypercube
        snv_norm: False


    # ----------------------------
    # IMAGE DOWNSAMPLING PARAMETERS
    downsampling:
        # Band selection, please specify the band range to be selected (list[int, int])
        # e.g. bd_downsampling: [10, 90]
        bd_downsampling:

        # Pixel downsampling
        pix_downsampling: True

        pix_ds_config: 
            # Proportion of pixels to keep ([0, 1] float)
            rate: 0.3

            # Specifying the set to which Downsampling is applied on (str)
            # Choose among: {'train', 'test', ''}
            # (by default, this is applied to both sets)
            set: train

            # Specifying the class to which downsampling is applied on (str)
            # (by default, it will be applied to every classes)
            class: no_damage

            # Transfer the pixels retained during the downsampling to the other set
            transfer: False

            # Specify the downsampling mode (str)
            # Choose among: {'random', 'disjoint'}
            mode: random

    # ----------------------------
    # DATA AUGMENTATION PARAMETERS
    data_augmentation:
        # Random flips
        flip_augmentation: False

        # Random radiation noise
        radiation_augmentation: False

        # Random mixes between spectra
        mixture_augmentation: False


# ---------------------------------------------------------------------------
# MODEL HYPERPARAMETERS
model:
    # Model name (str)
    # Choose among:{'SVM', 'SVM_grid', 'baseline', 'hu', 'hamida', 'lee', 'chen',
    #               'li', 'he', 'luo", 'sharma', 'boulch', 'liu', 'mou'}
    model_name: hamida

    # Path to the file containing the model weights to be restored. (str)
    # In the case of cross validation experiment, this path should lead
    # to the directory containing the different weights needed.
    #############################checkpoint: /home/nkusi/repos/HSI/DeepHyperX/deephyperx/output/Laser_hamida_cross_val2020-11-18_16:42:11.981344/checkpoint
    #checkpoint: /home/nkusi/repos/HSI/DeepHyperX/deephyperx/output/Laser_hamida_cross_val2020-11-30_10:02:18.885144/checkpoint
    #checkpoint: /home/nkusi/repos/HSI/DeepHyperX/deephyperx/output/Laser_hamida_cross_val2020-11-17_20:39:52.482316/checkpoint
    checkpoint: 

    # Activate full resume mode:
    # If True, load starting epoch, model states, optimizer states and setup
    # learning rate scheduler, this would be used for resuming a stopped training
    # If False, only the model state would be loaded, this would be used for fine-tuning
    # and load pre-trained models
    full_resume: False

    # Training epochs (int)
    epoch: 5 

    # Learning rate (float)
    learning_rate: 0.001

    # Batch size (int)
    batch_size: 1

    # Inverse median frequency class balancing
    class_balancing: True

    # Sliding window step stride during inference (int)
    test_stride: 1

    # Specify the sample patch size (int)
    # (by default, the patch size is defined by the default model input size)
    patch_size:
